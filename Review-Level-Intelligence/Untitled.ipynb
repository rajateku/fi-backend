{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3cc48055",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = [\"First off I can't find my home town in the app, This is useless I cannot save my pic. I live and grew up in a Small town in Ohio and I can find it therefore I cannot complete my profile\"]\n",
    "\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk import RegexpParser\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "58d57576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First ADV\n",
      "off ADV\n",
      "I PRON\n",
      "ca AUX\n",
      "n't PART\n",
      "n't\n",
      "find VERB\n",
      "my PRON\n",
      "home NOUN\n",
      "town NOUN\n",
      "in ADP\n",
      "the DET\n",
      "app NOUN\n",
      ", PUNCT\n",
      "This PRON\n",
      "is AUX\n",
      "useless ADJ\n",
      "I PRON\n",
      "can AUX\n",
      "not PART\n",
      "not\n",
      "save VERB\n",
      "my PRON\n",
      "pic NOUN\n",
      ". PUNCT\n",
      "I PRON\n",
      "live VERB\n",
      "and CCONJ\n",
      "grew VERB\n",
      "up ADP\n",
      "in ADP\n",
      "a DET\n",
      "Small ADJ\n",
      "town NOUN\n",
      "in ADP\n",
      "Ohio PROPN\n",
      "and CCONJ\n",
      "I PRON\n",
      "can AUX\n",
      "find VERB\n",
      "it PRON\n",
      "therefore ADV\n",
      "I PRON\n",
      "can AUX\n",
      "not PART\n",
      "not\n",
      "complete VERB\n",
      "my PRON\n",
      "profile NOUN\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "doc = nlp(s[0])\n",
    "\n",
    "tokens_tags = []\n",
    "for token in doc:\n",
    "    \n",
    "    # print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "    #         token.shape_, token.is_alpha, token.is_stop)\n",
    "    print(token.text, token.pos_)\n",
    "    tokens_tags.append((token.text, token.pos_ ))\n",
    "\n",
    "    if token.pos_ == \"PART\":\n",
    "        print(token.text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c8b960d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Split: ['I', 'can', 'not', 'add', 'my', 'location']\n",
      "After Token: [('I', 'PRP'), ('can', 'MD'), ('not', 'RB'), ('add', 'VB'), ('my', 'PRP$'), ('location', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "text =\"I can not add my location\".split()\n",
    "print(\"After Split:\",text)\n",
    "tokens_tag = pos_tag(text)\n",
    "print(\"After Token:\",tokens_tag)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33324c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S I/PRP can/MD not/RB add/VB my/PRP$ (mychunk location/NN))\n"
     ]
    }
   ],
   "source": [
    "patterns= \"\"\"mychunk:{<NN.?>*<VBD.?>*<JJ.?>*<CC>?}\"\"\"\n",
    "chunker = RegexpParser(patterns)\n",
    "print(\"After Regex:\",chunker)\n",
    "output = chunker.parse(tokens_tag)\n",
    "print(\"After Chunking\",output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7c649af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(actionchunk ca/AUX n't/PART find/VERB my/PRON home/NOUN town/NOUN)\n",
      "(actionchunk can/AUX not/PART save/VERB my/PRON pic/NOUN)\n",
      "(actionchunk can/AUX not/PART complete/VERB my/PRON profile/NOUN)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# print(\"After Token:\",tokens_tags)\n",
    "# chunkGram = r\"\"\"\n",
    "#     NegChunk1: {<DT>+<NN|NNP>+}\n",
    "#     NegChunk2: {<NN>+<VB.?>*<RB>+}\n",
    "#     YearsChunk: {<CD>+<CC|JJR>*<NNS|NN>+}\n",
    "#     DomainChunk1: {<IN|JJ>+<NN.?>+}\n",
    "#     DomainChunk2: {<NN.?>+<IN|JJ>+}\n",
    "#     DomainChunk3: {<NN.?>+<VB.?>+}\n",
    "#     DomainChunk4: {<VB.?>+<IN|DT>*<JJ|NN.?>+}\n",
    "#     DomainChunk5: {<NN.?>+<CC>*<NN.?>+}\n",
    "#     \"\"\"\n",
    "chunkGram = r\"\"\"\n",
    "    NegChunk1: {<DT>+<NN|NNP>+}\n",
    "    NegChunk2: {<NN>+<VB.?>*<RB>+}\n",
    "    \"\"\"\n",
    "\n",
    "patterns= \"\"\"actionchunk:{<AUX><PART><VERB><PRON|NOUN>*}\"\"\"\n",
    "chunker = RegexpParser(patterns)\n",
    "# print(\"After Regex:\",chunker)\n",
    "output = chunker.parse(tokens_tags)\n",
    "# print(\"After Chunking\",output)\n",
    "\n",
    "for subtree in output.subtrees():\n",
    "    if subtree.label() == 'actionchunk' and len(subtree)>1:\n",
    "        print(subtree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f5d68659",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# from nltk import pos_tag\n",
    "from nltk import RegexpParser\n",
    "\n",
    "# s = \"First off I can't find my home town in the app, This is useless I cannot save my pic. I live and grew up in a Small town in Ohio and I can find it therefore I cannot complete my profile\"\n",
    "\n",
    "\n",
    "def gieven_sentence_give_chunk(s):\n",
    "    doc = nlp(s)\n",
    "    tokens_tags = []\n",
    "    for token in doc:\n",
    "\n",
    "        tokens_tags.append((token.text, token.pos_))\n",
    "        print((token.text, token.pos_))\n",
    "\n",
    "        if token.pos_ == \"PART\":\n",
    "            print(token.text)\n",
    "\n",
    "#     patterns= \"\"\"actionchunk:{<AUX><PART><VERB><PRON|NOUN>*}\"\"\"\n",
    "    patterns= \"\"\"actionchunk:{<AUX><.*>*<PRON|NOUN>+}\"\"\"\n",
    "\n",
    "    chunker = RegexpParser(patterns)\n",
    "    output = chunker.parse(tokens_tags)\n",
    "\n",
    "    for subtree in output.subtrees():\n",
    "        if subtree.label() == 'actionchunk' and len(subtree)>1:\n",
    "            print(subtree)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fca9e403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('can', 'AUX')\n",
      "('not', 'PART')\n",
      "not\n",
      "('upload', 'VERB')\n",
      "('the', 'DET')\n",
      "('card', 'NOUN')\n",
      "(actionchunk can/AUX not/PART upload/VERB the/DET card/NOUN)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "s = \"would never accept the personâ€™s street address\"\n",
    "s = \"cannot upload the card\"\n",
    "gieven_sentence_give_chunk(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f465936c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
